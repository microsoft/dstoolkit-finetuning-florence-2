{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Florence-2 Model\n",
    "\n",
    "Florence-2 is a vision foundation model trained by Microsoft in 2023. It has good out of the box performance at a variety of image tasks. It produces predictions by outputing text tokens, with a limit of 1024 per image. To designate parts of the image, normalised x, y co-ordinates are produced by the model.\n",
    "\n",
    "This notebook details how to use Florence-2 and its base model. We need to specify both a task type and a specific prompt for further instructions.\n",
    "\n",
    "Task Types (a sample of the main tasks):\n",
    "- `<CAPTION>` Description\n",
    "- `<DETAILED_CAPTION>` Description\n",
    "- `<REGION_TO_DESCRIPTION>` Description, given a bounding box\n",
    "- `<OD>` Object detection\n",
    "- `<OCR_WITH_REGION>` Object detection, given bounding box\n",
    "- `<REFERRING_EXPRESSION_SEGMENTATION>` Segmentation, given text\n",
    "- `<REGION_TO_SEGMENTATION>` Segmentation, given bounding box\n",
    "- `<OCR>` Optical Character Recognition\n",
    "- `<OCR_WITH_REGION>` Optical Character Recognition, given bounding box\n",
    "\n",
    "Example prompts when adding a task:\n",
    "- VQA: What does the image describe?\n",
    "- VQA: What does the region {region} describe?\n",
    "- Object Detection: Locate the objects in the image.\n",
    "- Object Detection: Locate the phrases in the caption: {caption}.\n",
    "- Segmentation: What is the polygon mask of region {region}?\n",
    "- OCR: Extract text with region {region}.\n",
    "\n",
    "For full details of the training, see the paper: https://arxiv.org/abs/2311.06242"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from PIL import Image, ImageDraw, ImageOps\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# setup device and dtype if using GPU\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "print(f\"DEVICE: {DEVICE} \\nTORCH DTYPE: {torch_dtype}\")\n",
    "\n",
    "# setup correct file paths\n",
    "data_path = \"./data/\"\n",
    "\n",
    "# Load model and processor from Hugging Face\n",
    "# microsoft/Florence-2-large-ft is the fine-tuned version of microsoft/Florence-2-large. It is finetuned for a variety of downstream tasks.\n",
    "model_name = \"microsoft/Florence-2-large-ft\" # microsoft/Florence-2-large\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch_dtype, trust_remote_code=True).to(DEVICE)\n",
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create basic functions for a variety of image processing tasks\n",
    "\n",
    "def obj_det_Florence(classes, image, model, processor):\n",
    "    \"\"\"\n",
    "    Return all the bounding boxes and labels for the classes in the image.\n",
    "    \"\"\"\n",
    "    # we can combine as so: \n",
    "    # prompts = \"detect cars and motobikes in the image\"\n",
    "    # but if it cannot detect the object, it may be because of model's limitations in handling complex scenes with multiple objects. \n",
    "    # Therefore, its better to call one at a time to get the bounding boxes for each class\n",
    "    prompts = [f\"Locate {i}\" for i in classes]\n",
    "\n",
    "    all_boxes = []\n",
    "    all_labels = []\n",
    "    for prompt in prompts:\n",
    "        # Process the input\n",
    "        prompt = prompt.lower()\n",
    "        inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(DEVICE, dtype=torch_dtype)\n",
    "\n",
    "        # Generate predictions\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            pixel_values=inputs[\"pixel_values\"],\n",
    "            max_new_tokens=1024, # limit\n",
    "            early_stopping=False,\n",
    "            do_sample=False,\n",
    "            num_beams=3,\n",
    "        )\n",
    "\n",
    "        # Decode the predictions\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "\n",
    "        # Post-process the output\n",
    "        parsed_answer = processor.post_process_generation(generated_text, task=\"<OD>\", image_size=(image.width, image.height))\n",
    "        \n",
    "        # Collect results\n",
    "        all_boxes.extend(parsed_answer['<OD>'][\"bboxes\"])\n",
    "        all_labels.extend(parsed_answer['<OD>'][\"labels\"])\n",
    "\n",
    "    combined_results = {\"bboxes\": all_boxes, \"labels\": all_labels}\n",
    "    \n",
    "    return combined_results"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
